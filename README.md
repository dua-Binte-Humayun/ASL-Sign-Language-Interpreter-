# ASL Sign Language Interpreter  
AI + Human-Computer Interaction + Accessibility Project

## Overview
The ASL Sign Language Interpreter is an AI-driven and HCI-focused system designed to translate American Sign Language (ASL) hand gestures into text in real time.  
This project aims to enhance accessibility and inclusive communication for deaf and hard-of-hearing users using computer vision and machine learning techniques.

The system emphasizes:
- Accessible Human-Computer Interaction
- Real-time gesture recognition
- Inclusive design principles
- Healthcare and assistive technology relevance


## Motivation
Millions of deaf and hard-of-hearing individuals face communication barriers in healthcare, education, and daily interactions.  
This project addresses those challenges by providing an AI-powered assistive tool that enables seamless interaction between hearing and non-hearing users.

This work aligns strongly with **ubiquitous computing, intelligent interaction, and accessible HCI research**.


## Key Features
- Real-time ASL recognition via webcam
- Static hand sign recognition (alphabets)
- Text-based output interface
- Designed with accessibility-first HCI principles
- Modular and extensible architecture


## Technology Stack
- Python
- OpenCV
- TensorFlow / Keras
- NumPy
- MediaPipe (optional)
- Jupyter Notebook


## System Workflow
1. Capture hand gestures via webcam
2. Preprocess image frames
3. Extract hand features
4. Classify ASL gestures using trained ML model
5. Display recognized text in real time


## HCI & Accessibility Principles Applied
- Visibility of system status
- Immediate feedback
- Error tolerance
- Minimal cognitive load
- Inclusive design for differently-abled users
- WCAG-inspired accessibility considerations


## Dataset
- Static ASL alphabet images
- Custom-collected samples
- Publicly available ASL datasets (for academic use)

Dataset details are documented in `data/dataset_description.md`.


## Evaluation Methodology
The system was evaluated using:
- Task success rate
- Recognition accuracy
- Usability testing
- Accessibility feedback

Evaluation data and results are available in the `evaluation/` folder.


## Screenshots
Screenshots of the real-time detection system are provided in the `screenshots/` folder.


## Future Improvements
- Dynamic sign recognition (words & sentences)
- Voice output for bidirectional communication
- Mobile application integration
- Healthcare-focused ASL vocabulary
- Edge-device optimization for IoT environments


## Research Relevance
This project aligns with:
- Human-Computer Interaction
- Intelligent Sensing
- Assistive Technologies
- Healthcare AI Systems
- Ubiquitous Computing


## Author
**Dua Mirza**   
Research Interests: Accessible HCI, AI for Healthcare, Intelligent Interaction , Data Science


## License
This project is licensed for academic and research use.
